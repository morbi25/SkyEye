[general]
# Number of epochs between validations
val_interval = 1
# Number of steps before outputting a log entry
log_interval = 10
cudnn_benchmark = no
log_train_samples = yes

[base]
# Architecture for the body
base = efficientdet-d3
# Path to pre-trained weights
weights =
# Normalization mode:
# -- bn: in-place batch norm everywhere
# -- syncbn: synchronized in-place batch norm everywhere
# -- syncbn+bn: synchronized in-place batch norm in the static part of the network, in-place batch norm everywhere else
# -- gn: group norm everywhere
# -- syncbn+gn: synchronized in-place batch norm in the static part of the network, group norm everywhere else
# -- off: do not normalize activations (scale and bias are kept)
normalization_mode = syncbn
# Activation: 'leaky_relu' or 'elu'
activation = leaky_relu
activation_slope = 0.01
# Group norm parameters
gn_groups = 0
# Additional parameters for the body
base_params = {}
# Number of frozen modules: in [1, 5]
num_frozen = 0
# Wether to freeze BN modules
bn_frozen = no

[voxelgrid]
in_channels = {"efficientdet-d0": 64, "efficientdet-d3": 160}
in_feat_scale = 0.125
y_extents = (-17, 3)

[sem]
fpn_min_level = 0
fpn_levels = 4
pooling_size = (64, 64)
# Loss settings
fv_ohem = 0.25
bev_ohem = 0.25
fv_class_weights = (2.5539, 3.8968, 1.9405, 1.7729, 5.6612, 3.6329, 29.7894, 24.0978, 3.7109, 8.2586)
bev_class_weights = (2.5539, 3.8968, 1.9405, 5.6612, 29.7894, 24.0978, 3.7109, 8.2586)

[optimizer]
opt_algo = sgd
base_lr = 0.005
weight_decay = 0.0001
weight_decay_norm = no
momentum = 0.9
nesterov = yes
loss_weights = {"fv_sem_loss": 1, "bev_sem_loss": 1}

[scheduler]
epochs = 20
params = {"milestones": [15, 18], "gamma": [0.5, 0.2]}

# Scheduler type: 'linear', 'step', 'poly' or 'multistep'
type = multistep_multigamma
# When to update the learning rate: 'batch', 'epoch'
update_mode = epoch
# Additional parameters for the scheduler
# -- linear
#   from: initial lr multiplier
#   to: final lr multiplier
# -- step
#   step_size: number of steps between lr decreases
#   gamma: multiplicative factor
# -- poly
#   gamma: exponent of the polynomial
# -- multistep
#   milestones: step indicies where the lr decreases will be triggered
# -- multistep_multigamma
#   gamma: List containing the factor wrt the base_LR by which the LR decreases
#   lr[i] = base_lr * gamma[i]
burn_in_steps = 0
burn_in_start = 0.00333

[cameras]
intrinsics = {"front": {"fx": 552.554261, "fy": 552.554261, "px": 682.049453, "py": 238.769549}}
extrinsics = {"front": {"translation": (0.8, 0.3, 1.55), "rotation": (-85, 0, 180)}}
bev_params = {"f": 336, "cam_z": 25}

fv_extrinsics = {"translation": (0, 0, 0), "rotation": (-5.6416, 0, 0)}

[dataset]
fv_sky_index = 5
fv_veg_index = 3

[dataloader]
rgb_cameras = ("front")
# The total size of the window needed in ONE directions. Ex: 6 = 6 in one direction
total_window_size = 4
# The size of the window in ONE direction. Ex: 2 = 2 in one direction
fvsem_window_size = 4

# Batch size
train_batch_size = 2
val_batch_size = 1

# Image size parameters
shortest_size = 376
longest_max_size = 1408
# Augmentation parameters
rgb_mean = (0.485, 0.456, 0.406)
rgb_std = (0.229, 0.224, 0.225)
random_flip = no
scale = 1
bev_crop = (768, 704)
front_resize = (384, 1408)
random_brightness = (0.8, 1.2)
random_contrast = (0.8, 1.2)
random_saturation = (1, 1)
random_hue = (0, 0)

# Number of worker threads
train_workers = 2
val_workers = 2
# Subsets
train_set = train
val_set = val